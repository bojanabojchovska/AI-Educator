{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./req.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile models.py\n",
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# import torch\n",
    "# load_dotenv(find_dotenv(), override=True)\n",
    "# def get_llm_model(repo_id=\"microsoft/Phi-3-mini-4k-instruct\", task=\"conversational\", max_new_tokens=512, do_sample=False, repetition_penalty=1.03):\n",
    "#     llm = HuggingFaceEndpoint(\n",
    "#         timeout=600,\n",
    "#         repo_id=repo_id,\n",
    "#         task=task,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         do_sample=do_sample,\n",
    "#         repetition_penalty=repetition_penalty,\n",
    "#     )\n",
    "#     return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile document_loader.py\n",
    "# import os\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# def load_document(file):\n",
    "#     name, extension = os.path.splitext(file)\n",
    "#     if extension == \".pdf\":\n",
    "#         print(f\"Loading {file}\")\n",
    "#         loader = PyPDFLoader(file)\n",
    "#     else:\n",
    "#         print(\"Document format is not supported!\")\n",
    "#         return None\n",
    "#     data = loader.load()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile text_generation_utils.py\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# from langchain import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# from models import get_llm_model\n",
    "# from document_loader import load_document\n",
    "# import asyncio\n",
    "# import re\n",
    "# load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# def get_prompt_template(num_flashcards):\n",
    "#     prompt = PromptTemplate(\n",
    "#         template=f\"\"\"You are an expert at creating flashcards or question-answer pairs based on a given text. Generate a total of {num_flashcards} question-answer pairs.\n",
    "\n",
    "#         **Format the output like this:**\n",
    "#         Question 1;;;Answer 1\n",
    "#         Question 2;;;Answer 2\n",
    "\n",
    "#         The text is: {{text}}\n",
    "\n",
    "        \n",
    "#         \"\"\",\n",
    "#         input_variables=[\"text\"],\n",
    "#     )\n",
    "#     return prompt\n",
    "\n",
    "# async def get_generated_text(num_flashcards, file):\n",
    "#     llm = get_llm_model(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "#     prompt = get_prompt_template(num_flashcards)\n",
    "#     qa_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "#     text = await asyncio.to_thread(load_document, file)\n",
    "#     answer = await qa_chain.ainvoke({\"text\": text})\n",
    "#     answer = format_answer(answer['text'], num_flashcards)\n",
    "#     return answer\n",
    "\n",
    "# def format_answer(answer, num_flashcards):\n",
    "#     split_text = re.split(r'(Question|Answer) \\d;{1,3}', answer.strip())\n",
    "#     split_text = [s.strip() for s in split_text if s.strip()]\n",
    "#     pairs = []\n",
    "#     pair_counter = 1\n",
    "#     for i in range(0, len(split_text), 4):\n",
    "#         pairs.append({\n",
    "#             f\"Question {pair_counter}\": split_text[i + 1],\n",
    "#             f\"Answer {pair_counter}\": split_text[i + 3]\n",
    "#         })\n",
    "#         pair_counter += 1\n",
    "#     return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API_KEY: pcsk_7VBRMs_KHEiy6F1JFRY7cWGuQZZmJisWh4stjRpCEYHnAEWbCUfSfUzUtGA4JYC8Gvc83e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")\n",
    "index = pc.Index(name=\"books\")\n",
    "index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
